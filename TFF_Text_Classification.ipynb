{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlowFederated Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook attempts to simulate a federated learning workflow using a centralized dataset.\n",
    "\n",
    "The dataset is split equally amongst the clients (the current splitting has not been done with the intent of i.i.d client datasets and this condition would need to be added to the flow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the TFF example notebook `federated_learning_for_image_classification.ipynb`.\n",
    "https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading packages and checking if TFF has been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import math\n",
    "import collections\n",
    "import nest_asyncio\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Define re-usable print seperators.\n",
    "sep = '-' * 25\n",
    "sep_2 = '#' * 10\n",
    "sep_3 = '-' * 50\n",
    "sep_4 = '=' * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "%load_ext tensorboard\n",
    "np.random.seed(0)\n",
    "\n",
    "tff.federated_computation(lambda: 'Testing TFF.')()\n",
    "assert tf.executing_eagerly() == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting federated learning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of clients who's tuned weights will be included in the federated model updation.\n",
    "NUM_CLIENTS = 10\n",
    "\n",
    "# These parameters were used by the image classification notebook using tff.simulation dataset.\n",
    "NUM_EPOCHS = 5\n",
    "SHUFFLE_BUFFER = 100\n",
    "PREFETCH_BUFFER = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: ag_news_subset\n",
      "train_data.element_spec: (TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n",
      "type(train_data): <class 'tensorflow.python.data.ops.dataset_ops._OptionsDataset'>\n"
     ]
    }
   ],
   "source": [
    "#dataset_name = \"snli\"\n",
    "#dataset_name = \"imdb_reviews\"\n",
    "#dataset_name = \"sentiment140\"\n",
    "\n",
    "# `ag_news_subset` supervised dataset has 3 features: (`title`, `description`, `label`),\n",
    "# with feature `label` having 4 classes.\n",
    "dataset_name = \"ag_news_subset\"\n",
    "\n",
    "train_data, test_data= tfds.load(\n",
    "    name=dataset_name,\n",
    "    split=[\"train\", \"test\"], \n",
    "    with_info=False, \n",
    "    as_supervised=True,\n",
    "    shuffle_files=True\n",
    ")\n",
    "\n",
    "assert isinstance(train_data, tf.data.Dataset)\n",
    "assert isinstance(test_data, tf.data.Dataset)\n",
    "print('Using dataset:', dataset_name)\n",
    "print('train_data.element_spec:', train_data.element_spec)\n",
    "print('type(train_data):', type(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting dataset as list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = list(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting number of rows in dataset and batch size per client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROWS = len(train_data_list)\n",
    "BATCH_SIZE = math.floor(NUM_ROWS/NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from rows in train dataset (2 methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a function that returns row values extracted from the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_gen():\n",
    "    \"\"\"Generator that returns row features.\"\"\"\n",
    "    for row in train_data_list:\n",
    "        (x, y) = row\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(generator_train_data): <class 'tensorflow.python.data.ops.dataset_ops.FlatMapDataset'>\n"
     ]
    }
   ],
   "source": [
    "# Generator returns a dataset (from which client datasets are created).\n",
    "generator_train_data = tf.data.Dataset.from_generator(\n",
    "    train_data_gen, \n",
    "    output_types=(tf.string, tf.int64), \n",
    "    output_shapes=((), ())\n",
    ")\n",
    "\n",
    "print('type(generator_train_data):', type(generator_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using Tensor Slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(tensorslice_train_data): <class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n"
     ]
    }
   ],
   "source": [
    "# Extract row features and append to feature list.\n",
    "description, label = [], []\n",
    "for row in train_data:\n",
    "    description.append(row[0])\n",
    "    label.append(row[1])\n",
    "\n",
    "# Create dataset from tensor slices.\n",
    "tensorslice_train_data = tf.data.Dataset.from_tensor_slices((description, label))\n",
    "print('type(tensorslice_train_data):', type(tensorslice_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using extracted data to create Client datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from TensorSliceDataset data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_dict(dataset, name, \n",
    "                       show=True, num_clients=NUM_CLIENTS, \n",
    "                       batch_size=BATCH_SIZE, num_rows=NUM_ROWS):\n",
    "    \"\"\"Takes in extracted data and returns a dict containing client datasets.\"\"\"\n",
    "    client_dict = {}\n",
    "    if show:\n",
    "        print('Name of dataset:', name)\n",
    "        print(\"Num of rows:\", num_rows)\n",
    "        print(\"Client batch size:\", batch_size)\n",
    "        print(sep)\n",
    "    batched_data = list(dataset.batch(batch_size))\n",
    "    for i in range(num_clients):\n",
    "        key = \"client_\" + str(i)\n",
    "        client_dict[key] = tf.data.Dataset.from_tensor_slices(batched_data[i])\n",
    "    return client_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_format_fn(element, show=False):\n",
    "    \"\"\"Return the features as an `OrderedDict`.\"\"\"\n",
    "    if show:\n",
    "        print('preprocess:', element['description'], element['label'])\n",
    "    return collections.OrderedDict(\n",
    "        x=tf.reshape(element['description'], [-1, 1]),\n",
    "        y=tf.reshape(element['label'], [-1, 1])\n",
    "    )\n",
    "\n",
    "def preprocess(dataset):\n",
    "    return dataset.map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
    "    #return batch_format_fn(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preprocessing has been commented out as this was pertaining to re-shaping image data.\n",
    "# (There could be mapping functions used which are necessary to load the data correctly, \n",
    "# please refer to the image classification counterpart function)\n",
    "def make_federated_data(client_data, client_ids):\n",
    "    \"\"\"Returns client datasets as a list.\"\"\"\n",
    "    return [\n",
    "        client_data[x]\n",
    "        #preprocess(client_data[x])\n",
    "        for x in client_ids\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create client dataset dictionaries from the extracted data (using the 2 methods mentioned above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of dataset: tensorslice\n",
      "Num of rows: 120000\n",
      "Client batch size: 12000\n",
      "-------------------------\n",
      "Name of dataset: generator\n",
      "Num of rows: 120000\n",
      "Client batch size: 12000\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "client_datasets_tensor = create_client_dict(tensorslice_train_data, name='tensorslice')\n",
    "client_datasets_generator = create_client_dict(generator_train_data, name='generator')\n",
    "\n",
    "# Check if dict keys are the same.\n",
    "assert client_datasets_tensor.keys() == client_datasets_generator.keys()\n",
    "client_list = client_datasets_tensor.keys()\n",
    "\n",
    "# Check the datasets.\n",
    "for x in client_list:\n",
    "    assert type(client_datasets_tensor[x]) == type(client_datasets_generator[x])\n",
    "    assert len(client_datasets_tensor[x]) == len(client_datasets_generator[x])\n",
    "    assert len(client_datasets_tensor[x]) == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek_client_data(client_datasets, num_clients=1):\n",
    "    \"\"\"Prints the 1st row values (w/ reshaping) for the 1st `num_clients` clients.\"\"\"\n",
    "    for client_index, key in enumerate(client_datasets.keys()):\n",
    "        if client_index == num_clients:\n",
    "            break\n",
    "        print(key, 'has', len(list(client_datasets[key])), 'rows.\\n')\n",
    "        for index, row in enumerate(client_datasets[key]):\n",
    "            (x, y) = row\n",
    "            if index == 0:\n",
    "                print('1st row:', row)\n",
    "                print(sep)\n",
    "                print('x:', x)\n",
    "                print('type(x):', type(x))\n",
    "                print(sep_2)\n",
    "                x_reshaped = tf.reshape(x, [-1, 1])\n",
    "                print('reshaped x:', x_reshaped)\n",
    "                print('type(reshaped x):', type(x_reshaped))\n",
    "                print(sep)\n",
    "                print('y:', y)\n",
    "                print('type(y):', type(y))\n",
    "                print(sep_2)\n",
    "                y_reshaped = tf.reshape(y, [-1, 1])\n",
    "                print('reshaped y:', y_reshaped)\n",
    "                print('type(reshaped y):', type(y_reshaped))\n",
    "                print(sep_4, '\\n')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment and run to print the 1st row values (w/ reshaping) of the 1st client.\n",
    "#peek_client_data(client_datasets_tensor)\n",
    "#peek_client_data(client_datasets_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "federated_train_data_tensor[0]: <TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)>\n",
      "federated_train_data_generator[0]: <TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)>\n",
      "-------------------------\n",
      "Number of client datasets: 10\n",
      "First dataset: <TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int64)>\n",
      "element_spec: (TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n"
     ]
    }
   ],
   "source": [
    "# Generate federated datasets using the client dataset dictionaries \n",
    "# created from the 2 extraction methods.\n",
    "federated_train_data_tensor = make_federated_data(client_datasets_tensor, client_list)\n",
    "federated_train_data_generator = make_federated_data(client_datasets_generator, client_list)\n",
    "\n",
    "assert len(federated_train_data_tensor) == NUM_CLIENTS\n",
    "assert len(federated_train_data_tensor) == len(federated_train_data_generator)\n",
    "assert format(federated_train_data_tensor[0]) == format(federated_train_data_generator[0])\n",
    "assert federated_train_data_tensor[0].element_spec == federated_train_data_generator[0].element_spec\n",
    "\"\"\"\n",
    "# Throws `AssertionError` but upon printing they seem same.\n",
    "# I suspect this could also be causing an error.\n",
    "\n",
    "assert federated_train_data_tensor[0] == federated_train_data_generator[0]\n",
    "\"\"\"\n",
    "print('federated_train_data_tensor[0]:', federated_train_data_tensor[0])\n",
    "print('federated_train_data_generator[0]:', federated_train_data_generator[0])\n",
    "print(sep)\n",
    "print('Number of client datasets: {l}'.format(l=len(federated_train_data_tensor)))\n",
    "print('First dataset: {d}'.format(d=federated_train_data_tensor[0]))\n",
    "print('element_spec: {d}'.format(d=federated_train_data_tensor[0].element_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created using reference https://keras.io/examples/nlp/text_classification_from_scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    \"\"\"Function that applies standardization in TextVectorization layer.\"\"\"\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )\n",
    "\n",
    "def create_keras_model():\n",
    "    max_features = 20000\n",
    "    embedding_dim = 128\n",
    "    sequence_length = 500\n",
    "    \n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(1,), dtype=tf.string),\n",
    "        tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            standardize=custom_standardization,\n",
    "            max_tokens=max_features,\n",
    "            output_mode=\"int\",\n",
    "            output_sequence_length=sequence_length,),\n",
    "        tf.keras.layers.Embedding(max_features, embedding_dim),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3),\n",
    "        tf.keras.layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "        tf.keras.layers.Softmax(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    keras_model = create_keras_model()\n",
    "    print(keras_model.summary())\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=federated_train_data_tensor[0].element_spec,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on federated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 165, 128)          114816    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 53, 128)           114816    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,806,273\n",
      "Trainable params: 2,806,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected tensorflow.python.ops.variables.Variable, found tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-424167c257ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m iterative_process = tff.learning.build_federated_averaging_process(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mclient_optimizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mserver_optimizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow_federated/python/learning/federated_averaging.py\u001b[0m in \u001b[0;36mbuild_federated_averaging_process\u001b[0;34m(model_fn, client_optimizer_fn, server_optimizer_fn, client_weighting, broadcast_process, aggregation_process, model_update_aggregation_factory, use_experimental_simulation_loop)\u001b[0m\n\u001b[1;32m    262\u001b[0m                         use_experimental_simulation_loop)\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m   iter_proc = optimizer_utils.build_model_delta_optimizer_process(\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m       \u001b[0mmodel_to_client_delta_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_fed_avg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py\u001b[0m in \u001b[0;36mbuild_model_delta_optimizer_process\u001b[0;34m(model_fn, model_to_client_delta_fn, server_optimizer_fn, broadcast_process, aggregation_process, model_update_aggregation_factory)\u001b[0m\n\u001b[1;32m    629\u001b[0m   \u001b[0mpy_typecheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_optimizer_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m   \u001b[0mmodel_weights_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_type_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbroadcast_process\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow_federated/python/learning/model_utils.py\u001b[0m in \u001b[0;36mweights_type_from_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mpy_typecheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtype_conversions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_from_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelWeights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow_federated/python/learning/model_utils.py\u001b[0m in \u001b[0;36mfrom_model\u001b[0;34m(cls, model)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfrom_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mpy_typecheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_trainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow_federated/python/learning/model_utils.py\u001b[0m in \u001b[0;36mnon_trainable_variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnon_trainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_check_iterable_of_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_trainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow_federated/python/learning/model_utils.py\u001b[0m in \u001b[0;36m_check_iterable_of_variables\u001b[0;34m(variables)\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0mpy_typecheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mpy_typecheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow_federated/python/common_libs/py_typecheck.py\u001b[0m in \u001b[0;36mcheck_type\u001b[0;34m(target, type_spec, label)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \"\"\"\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     raise TypeError('Expected {}{}, found {}.'.format(\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;34m'{} to be of type '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         type_string(type_spec), type_string(type(target))))\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected tensorflow.python.ops.variables.Variable, found tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler."
     ]
    }
   ],
   "source": [
    "iterative_process = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(iterative_process.initialize.type_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER_STATE, FEDERATED_DATA -> SERVER_STATE, TRAINING_METRICS\n",
    "state, metrics = iterative_process.next(state, federated_train_data)\n",
    "print('round  1, metrics={}'.format(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, metrics = iterative_process.next(state, federated_train_data)\n",
    "print('round  1, metrics={}'.format(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROUNDS = 11\n",
    "for round_num in range(2, NUM_ROUNDS):\n",
    "    state, metrics = iterative_process.next(state, federated_train_data)\n",
    "    print('round {:2d}, metrics={}'.format(round_num, metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying model metrics in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"./.tmp/logs/scalars/training/\"\n",
    "summary_writer = tf.summary.create_file_writer(logdir)\n",
    "state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with summary_writer.as_default():\n",
    "    for round_num in range(1, NUM_ROUNDS):\n",
    "        state, metrics = iterative_process.next(state, federated_train_data)\n",
    "        for name, value in metrics['train'].items():\n",
    "            tf.summary.scalar(name, value, step=round_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {logdir}\n",
    "%tensorboard --logdir {logdir} --port=0\n",
    "\n",
    "# !rm -R ./.tmp/logs/scalars/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aebrain_aman)",
   "language": "python",
   "name": "aebrain_aman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
