{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b099e6",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets tensorboardx transformers\n",
    "\n",
    "# Clone the repository containing simulation code.\n",
    "#!git clone https://github.com/MFreidank/Federated_Learning_Workshop_AMLD2021.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f258e91",
   "metadata": {},
   "source": [
    "## Set up simulation environment\n",
    "Using:\n",
    "- `torch` and `numpy` to perform modelling.\n",
    "- `tensorboardx` to graph training.\n",
    "- `pickle` to save trained models.\n",
    "\n",
    "#### Load `torch` based Federated Learning (FL) functions for our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f82a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load python packages\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import path, getcwd, makedirs\n",
    "from itertools import islice\n",
    "from tqdm import tqdm, trange\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Append `torch_federated_learning` package path.\n",
    "module_path = path.abspath(path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(path.join(module_path, \"src/torch_federated_learning/\"))\n",
    "# Import FL simulation functions from above directory. \n",
    "from options import (\n",
    "    args_parser,\n",
    "    get_nb_args,\n",
    "    print_train_stats,\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    get_dataset, \n",
    "    num_batches_per_epoch,\n",
    "    average_weights, \n",
    "    exp_details,\n",
    ")\n",
    "from update import (\n",
    "    test_inference,\n",
    "    ClientShard,\n",
    ")\n",
    "from models import (\n",
    "    get_model, \n",
    "    get_optimizer,\n",
    ")\n",
    "# Define path and directory to save trained model and graphs.\n",
    "save_path = \"../save/\"\n",
    "model_save_dir = \"objects\"\n",
    "# Load default arguments.\n",
    "args = get_nb_args()\n",
    "# Create directory to save trained model weight `.pkl` (pickle) files.\n",
    "makedirs(path.join(save_path, model_save_dir), exist_ok=True)\n",
    "# Define paths\n",
    "path_project = path.abspath('..')\n",
    "logger = SummaryWriter('../logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ab041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended to use cpu, use GPU when actually performing model training.\n",
    "#args.gpu = 'cuda:0'\n",
    "\n",
    "if args.gpu:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "device = 'cuda' if args.gpu else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc53a019",
   "metadata": {},
   "source": [
    "#### Framework used\n",
    "`PyTorch` (loaded as `torch`) is an open source machine learning library  \n",
    "that specializes in tensor computations and GPU acceleration.  \n",
    "\n",
    "We will be simulating a FL image classification task using the `torch` framework. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723a2a1b",
   "metadata": {},
   "source": [
    "### Start of with CV (img classification) {short}\n",
    "\n",
    "### Introduce what we are doing with:\n",
    "1. baseline(traditional ML; centralized): {short}  \n",
    "- Why traditional and it's cons (data) {long}  \n",
    "2. FL with single client shard:\n",
    "- Biggest centralizable dataset\n",
    "- \n",
    "3. FL: Contrast with baseline, Pros, {long}  \n",
    "\n",
    "### Introduce NLP based implementation and methods  \n",
    "- (ADE:sensitive) {short}\n",
    "- FL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6986d8",
   "metadata": {},
   "source": [
    "## Part 1: Traditional Computer Vision (CV) training\n",
    "For this simulation we use a 6 layer Convolutional Neural Network (CNN) model,  \n",
    " consisting of 2 convolutional layers, 1 pooling layer and 3 dense layers.  \n",
    "   [Image of CNN model]  \n",
    "\n",
    "We train this model on the CIFAR10 dataset which with 6k images per class.  \n",
    "   [Image depicting overview of CIFAR10]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e375e6",
   "metadata": {},
   "source": [
    "### Load model and dataset splits\n",
    "Using the entire CIFAR10 dataset we make 2 splits: Train and Test in a 50/10 ratio.  \n",
    "\n",
    "The train set will thus have 50k images.  \n",
    "\n",
    "The test set will have 10k images held out from both baseline and Federated Learning simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parse arguments to simulate computer vision.\n",
    "args = get_nb_args(task=\"cv\")\n",
    "\n",
    "# Load the CNN model.\n",
    "args.model = 'cnn'\n",
    "model = get_model(args)\n",
    "# Load CIFAR10 dataset stages.\n",
    "args.dataset = 'cifar'\n",
    "train_dataset, test_dataset, _ = get_dataset(args)\n",
    "# Set the model to train and send it to device.\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b743b",
   "metadata": {},
   "source": [
    "### Set optimizer and criterion\n",
    "For the traditional training flow we use:\n",
    "- Stochastic Gradient Descent (SGD) optimizer to train the CNN model.\n",
    "- Learning rate of 0.01 for training.\n",
    "- Negative log likelihood loss (criterion) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer used during model training.\n",
    "args.optimizer = 'sgd'\n",
    "args.lr = 0.01\n",
    "optimizer = get_optimizer(args=args, model=model)\n",
    "# Set criterion (loss function) to device.\n",
    "criterion = torch.nn.NLLLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be1187f",
   "metadata": {},
   "source": [
    "### Use `DataLoader` to create `trainloader`\n",
    "Using the `torch.DataLoader` object we can create batches for the training loop.  \n",
    "\n",
    "Setting a `batch_size` of 64 i.e. 64 images per batch to train the model.  \n",
    "\n",
    "We also `shuffle` the training data before generating batches for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using torch.DataLoader, load the training dataset into batches.\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9b941f",
   "metadata": {},
   "source": [
    "### Commence model training\n",
    "Using the `trainloader` which contains generated batches of data,  \n",
    "we begin the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0684ab40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set number of epochs to train model on.\n",
    "args.epochs = 10\n",
    "# Initiate a list to store epoch losses.\n",
    "epoch_loss = []\n",
    "# Iterate through \n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    # Initiate a list to store losses for every batch.\n",
    "    batch_loss = []\n",
    "    # Iterate through batches and for every batch, \n",
    "    # perform training pass (forward and backward pass) on CNN model.\n",
    "    for batch_idx, (images, labels) in enumerate(trainloader):\n",
    "        # Load the batch images and labels to the device.\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # Perform forward pass thorugh model.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        # Retrieve loss for the batch.\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Using loss value, perform a backward pass on the model.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # For every 50th batch, print training status and loss value.\n",
    "        if batch_idx % 50 == 0:\n",
    "            print_train_stats(args=args, epoch=epoch, \n",
    "                              batch_idx=batch_idx, images=images, \n",
    "                              trainloader=trainloader, loss=loss, \n",
    "                              flow_type='baseline')\n",
    "        # Append batch loss to list.\n",
    "        batch_loss.append(loss.item())\n",
    "    # Retrieve average loss on all batches for this epoch. \n",
    "    loss_avg = sum(batch_loss) / len(batch_loss)\n",
    "    # Append epoch loss (average batch loss) to list.\n",
    "    epoch_loss.append(loss_avg)\n",
    "\n",
    "# Graph training loss, train accuracy across epochs using `matplotlib`.\n",
    "plt.figure()\n",
    "plt.plot(range(len(epoch_loss)), epoch_loss)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Train loss')\n",
    "plt.savefig('../save/{}_nn_{}_{}_{}.png'.format(\n",
    "    args.task, args.dataset, args.model, args.epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c815244",
   "metadata": {},
   "source": [
    "### Results of training \n",
    "Training a single CNN model on 50k CIFAR10 images yielded the following loss curve.    \n",
    "\n",
    "<img src=\"../imgs/cv_nn_cifar_cnn_10.png\" width=\"400\" height=\"400\" />\n",
    "\n",
    "We see the training loss has decreased from ~2 to ~0.8 in 10 epochs of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce38bf0e",
   "metadata": {},
   "source": [
    "### Evaluate trained model performance using test split\n",
    "We can now evaluate the performance of this single model on the 10% heldout data => 10k images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa023c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = test_inference(args=args, model=model, \n",
    "                                     test_dataset=test_dataset, device=device)\n",
    "print('Test on', len(test_dataset), 'samples')\n",
    "print(\"Test Accuracy: {:.2f}%\".format(100*test_acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60016f5",
   "metadata": {},
   "source": [
    "We get Test Accuracy: ~56% with the traditional CV training on CIFAR10 dataset. \n",
    "\n",
    "Thus using the traditional Machine Learning flow we were able to train and evaluate a model on a downstream task with relative ease.  \n",
    "\n",
    "Having all the training and test data centralized allows us to easily tune hyperparameters and evaluate the changes using the test set.\n",
    "\n",
    "It also allows for better feature extraction and richer data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf05d1d",
   "metadata": {},
   "source": [
    "### Obstacles in using a traditional supervised ML flow\n",
    "Simulated datasets only go so far to demonstrate system capabilities in the wild.\n",
    "\n",
    "Though these datasets are considerably large in size,  \n",
    "data drift may occur by the time the records have been manually annotated.\n",
    "\n",
    "Another hurdle is that these datasets (though coming from real world) are usually collected by a single party or organization and may be suscept to bias compared to similar data in the wild."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4186d",
   "metadata": {},
   "source": [
    "## Part 2: Simulating training a model on a client-data shard\n",
    "We now simulate a server sending a global model to a client to be trained on it's data shard.  \n",
    "\n",
    "We will then send this localloy trained model to other clients for inference to evaluate it's performance.\n",
    "\n",
    "Lastly, we will evaluate this single-client trained model on the held-out test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9712730",
   "metadata": {},
   "source": [
    "### Load global model and dataset splits\n",
    "We load the same CIFAR10 dataset and generate train and test splits identical to the tradional ML flow so that we may compare both the approaches on the same test split.\n",
    "\n",
    "We also load the inital weights of the global model which will be trained on the client data shards.  \n",
    "[Image showing global model, client update and global averaging]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9df26c",
   "metadata": {},
   "source": [
    "#### Simulating FL clients\n",
    "We will simulate a FL environment assuming data is split identically and independently (i.i.d).  \n",
    "\n",
    "Creating 50 clients each having 50k/50 =>1k images in their data shards.  \n",
    "\n",
    "For the initial FL training flow let us the train the global model on only a single client shard for 10 epochs for one global round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here user groups is a dictionary containing keys for every member (here 50) \n",
    "# with subsequent values containing row IDs allocated to that client shard.  \n",
    "\n",
    "# Setting 50 clients.\n",
    "args.num_users = 50\n",
    "# Setting only 1 global round\n",
    "args.epochs = 1\n",
    "# Setting 10 epochs on client shard.\n",
    "args.local_ep = 10\n",
    "# Setting `frac` to use only one client shard\n",
    "args.frac = 0.02 # 50 clients x 0.02 sampling fraction => 1 client\n",
    "\n",
    "# Load dataset splits and user groups.\n",
    "train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "# Load global CNN CV model.\n",
    "global_model = get_model(args=args, img_size=train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98347c21",
   "metadata": {},
   "source": [
    "### Prepare FL training flow simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621086cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to train and send it to device.\n",
    "global_model.to(device)\n",
    "# Set the model to train mode \n",
    "# (i.e. green flag for model weights to be updated in backward pass).\n",
    "global_model.train()\n",
    "# Instatiate a copy of the untrained model weights to be used for Federated Averaging as a base.  \n",
    "global_weights = global_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8751832",
   "metadata": {},
   "source": [
    "### Simulating client data shard training\n",
    "Since we are simulating Federated Learning on a single device,  \n",
    "\n",
    "We use the `ClientShard` object to simulate a client data shard.\n",
    "\n",
    "We then use `ClientShard.update_weights` function to train the global model on this client chard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c055224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rondomly sample dataset to create client data shards.\n",
    "idxs_users = np.random.choice(range(args.num_users), 1, replace=False)\n",
    "# Get our brave clients id.\n",
    "lonely_client_idx = idxs_users[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53631310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We instatiate our pioneer single client using the idxs returned by the i.i.d sampler.\n",
    "lonely_client = ClientShard(args=args, \n",
    "                            dataset=train_dataset,\n",
    "                            client_idx=0, #Dummy ID for printing.\n",
    "                            idxs=user_groups[lonely_client_idx],\n",
    "                            device=device,\n",
    "                            logger=logger)\n",
    "# We can now pass the global model to the client for training, \n",
    "# it returns the updated local model and the local training loss.\n",
    "updated_local_model, loss = lonely_client.update_weights(\n",
    "    # Here we are passing a copy of the global model to the trainer.\n",
    "    model=copy.deepcopy(global_model), \n",
    "    # Only used for printing purpose.\n",
    "    global_round=1,\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c920a37",
   "metadata": {},
   "source": [
    "### Evaluating local model on all clients\n",
    "We now evaluate the average accuracy of the client-trained model on all 50 client-data shards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53580414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate avg training accuracy over local trained model.\n",
    "list_acc, list_loss = [], []\n",
    "# Set local model to evaluation mode.\n",
    "updated_local_model.eval()\n",
    "# Iterate through all clients.\n",
    "for idx in range(args.num_users):\n",
    "    # Instantiate clinet object using sampled IDs.\n",
    "    client_shard = ClientShard(args=args,\n",
    "                               dataset=train_dataset,\n",
    "                               idxs=user_groups[idx],\n",
    "                               logger=logger)\n",
    "    # Perform inferencing and evaluate accuracy on client shard.\n",
    "    acc, loss = client_shard.inference(model=updated_local_model)\n",
    "    # Append client accuracy and loss.\n",
    "    list_acc.append(acc)\n",
    "    list_loss.append(loss)\n",
    "# Get average accuracy across all 50 clients.\n",
    "train_accuracy.append(sum(list_acc) / len(list_acc))\n",
    "print(f' \\nAvg Training Stats after 1 global round:')\n",
    "print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "print('Train Accuracy: {:.2f}% \\n'.format(100 * train_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db621041",
   "metadata": {},
   "source": [
    "### Evaluate client trained local model on test split\n",
    "We can evaluate the local trained model on the heldout 10% test split => 10k images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference after completion of training\n",
    "test_acc, test_loss = test_inference(args=args, \n",
    "                                     model=updated_local_model, \n",
    "                                     test_dataset=test_dataset)\n",
    "print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f1d767",
   "metadata": {},
   "source": [
    "We thus simulated how the server would: \n",
    "- Send a model to a client shard to be trained.\n",
    "- Send the locallly trained model to be evaluated across client shards and on the held-out test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cad2c6",
   "metadata": {},
   "source": [
    "## Part 3: Simulate a complete FL training flow\n",
    "We saw how the server can send a model to clients for training and/or inferencing.\n",
    "\n",
    "Let us now add some more clients to the mix and train a global model using multiple clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108b3eb",
   "metadata": {},
   "source": [
    "### Load global model and dataset splits\n",
    "We load the global model and sample the data to create client shards as we did in Part 2 for the single client.  \n",
    "\n",
    "This time let us simulatre 50 clients and use 5 of them for every round of global model training.\n",
    "\n",
    "We will:\n",
    "- Train the global model on the client shards for 10 epochs.  \n",
    "- Use FedAVG to generate an averaged-model using all the client trained models.\n",
    "- Evaluate the averaged global model on other client shards.\n",
    "- After multiple global rounds of training, we will evaluate the global model on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90418402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 50 clients.\n",
    "args.num_users = 50\n",
    "# Setting 10 global rounds\n",
    "args.epochs = 10\n",
    "# Setting 10 epochs on client shard.\n",
    "args.local_ep = 10\n",
    "# Setting `frac` to use only one client shard\n",
    "args.frac = 0.1 # 50 clients x 0.1 sampling fraction => 5 clients\n",
    "\n",
    "# Load dataset and user groups.\n",
    "train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "# Load CV model.\n",
    "global_model = get_model(args=args, img_size=train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77e8c0",
   "metadata": {},
   "source": [
    "### Prepare FL training flow simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to train and send it to device.\n",
    "global_model.to(device)\n",
    "# Set the model to train mode \n",
    "# (i.e. green flag for model weights to be updated in backward pass).\n",
    "global_model.train()\n",
    "# Instatiate a copy of the untrained model weights to be used for Federated Averaging as a base.  \n",
    "global_weights = global_model.state_dict()\n",
    "# Instantiate lists to staore global client trained models and losses.\n",
    "global_train_loss, global_train_accuracy = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49379ed7",
   "metadata": {},
   "source": [
    "### Performing FedAvg training\n",
    "Now we train a global model on mulitple client-shards and perform FedAVG on the client-trained models.\n",
    "\n",
    "We do this for 10 global rounds of training and FedAVG'ing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825baeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_details(args)\n",
    "for epoch in range(args.epochs):\n",
    "    local_weights, local_losses = [], []\n",
    "    global_model.train()\n",
    "    # Randomly sample a fraction of clients and retrieve their ids.\n",
    "    user_frac = max(int(args.frac * args.num_users), 1)\n",
    "    idxs_users = np.random.choice(range(args.num_users), \n",
    "                                  user_frac, replace=False)\n",
    "    \n",
    "    ### Start of client-shard training {{{\n",
    "    for hidden_client_idx, idx in enumerate(idxs_users):\n",
    "        client_shard = ClientShard(args=args,\n",
    "                                   client_idx=hidden_client_idx,\n",
    "                                   dataset=train_dataset,\n",
    "                                   idxs=user_groups[idx],\n",
    "                                   logger=logger,\n",
    "                                   device=device)\n",
    "        updated_local_model, loss = client_shard.update_weights(\n",
    "            model=copy.deepcopy(global_model), \n",
    "            global_round=epoch\n",
    "        )\n",
    "        local_weights.append(copy.deepcopy(updated_local_model))\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "    ### }}} End of client-shard training\n",
    "\n",
    "    # Here we take a list of client-trained models and \n",
    "    # perform FedAvg to return a single avreagred global model.\n",
    "    global_weights = average_weights(local_weights)\n",
    "    # Update global weights with the averaged model weights.\n",
    "    global_model.load_state_dict(global_weights)\n",
    "    \n",
    "    # Calculate average client-training accuracy.\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # Calculate avg training accuracy over all users at every epoch\n",
    "    list_acc, list_loss = [], []\n",
    "    global_model.eval()\n",
    "    \n",
    "    # Remove trained clients from heldout evaluation.\n",
    "    heldout_clients = list(range(args.num_users))\n",
    "    for train_client_idx in list(idxs_users):\n",
    "        heldout_clients.remove(train_client_idx)\n",
    "    \n",
    "    ### Start of client evaluation {{{\n",
    "    for heldout_client_idx, idx in tqdm(enumerate(heldout_clients), \n",
    "                                        desc='Evaluating: Hidden client num:', \n",
    "                                        total=len(heldout_clients)):\n",
    "        client_shard = ClientShard(args=args,\n",
    "                                   client_idx=heldout_client_idx,\n",
    "                                   dataset=train_dataset,\n",
    "                                   idxs=user_groups[idx],\n",
    "                                   logger=logger,\n",
    "                                   device=device)\n",
    "        acc, loss = client_shard.inference(model=global_model)\n",
    "        list_acc.append(acc)\n",
    "        list_loss.append(loss)\n",
    "    train_accuracy.append(sum(list_acc) / len(list_acc))\n",
    "    ### }}} End of client evaluation\n",
    "    \n",
    "    # print global training loss after every round.\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f' \\nAvg Training Stats after {epoch + 1} global rounds:')\n",
    "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "        print('Train Accuracy: {:.2f}% \\n'.format(100 * train_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d0a39f",
   "metadata": {},
   "source": [
    "We thus trained a global model for 10 global rounds on 5 clients per round with 10 epochs per training client-shard.  \n",
    "\n",
    "Looking at the global avergae training loss we see some convergence:\n",
    "<img src=\"../imgs/fed_cifar_cnn_10_C[0.1]_iid[1]_E[10]_B[8]_loss.png\" width=\"350\" height=\"350\" />\n",
    "\n",
    "We also see that the Training accuracy on held-out clients has improved with global rounds.\n",
    "<img src=\"../imgs/fed_cifar_cnn_10_C[0.1]_iid[1]_E[10]_B[8]_acc.png\" width=\"350\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78527636",
   "metadata": {},
   "source": [
    "### Evaluate global trained model on test split\n",
    "We can evaluate the global trained model on the heldout 10% test split => 10k images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ffa512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference after completion of training\n",
    "test_acc, test_loss = test_inference(args=args, model=global_model, \n",
    "                                     test_dataset=test_dataset, device=device)\n",
    "print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e96a0f",
   "metadata": {},
   "source": [
    "We see that the Avg. Train Accuracy is around ~48% and the Test Acuracy is ~47%.\n",
    "\n",
    "We see that adding more clients (and subsequently more data has led to improvements in the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e6b2f",
   "metadata": {},
   "source": [
    "## Part 4: Performing FL for Natural Languge Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf9700",
   "metadata": {},
   "source": [
    "### ADE_corpus_v2\n",
    "To demonstrate Federate learning on NLP, we use the ADE Adverse Event (AE) classification open-source dataset.  \n",
    "\n",
    "The dataset contains 23516 records with:  \n",
    "16695 Non-AE related text &  \n",
    "6821 AE related text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd Federated_Learning_Workshop_AMLD2021 && python src/torch_federated_learning/federated_main.py \\\n",
    "--num_users=50 \\\n",
    "--frac=0.02 \\\n",
    "--task=nlp \\\n",
    "--model=distilbert \\\n",
    "--epochs=10 \\\n",
    "--local_ep=10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
